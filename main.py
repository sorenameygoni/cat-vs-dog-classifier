{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0df13-a78f-463d-81e1-17968ad3416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import kornia.augmentation as K\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079dab05-bdd2-4c7f-b9c8-9ad234e1513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = \"C:/Users/Msi/Desktop/CODE/tamrinecatdog/Test\"\n",
    "trn_dir = \"C:/Users/Msi/Desktop/CODE/tamrinecatdog/PetImages\"        \n",
    "vl_dir  = \"C:/Users/Msi/Desktop/CODE/tamrinecatdog/ValidationImages\"\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class Agumentation(nn.Module):\n",
    "    def __init__(self, train_mode=True):\n",
    "        super().__init__()\n",
    "        self.train_mode = train_mode\n",
    "        self.agument = nn.Sequential(\n",
    "            K.RandomHorizontalFlip(),\n",
    "            K.RandomRotation(degrees=20),\n",
    "            K.ColorJitter(brightness=0.17, contrast=0.12, saturation=0.14),\n",
    "            K.RandomGaussianBlur(kernel_size=(3,3), sigma=(0.1, 0.2)),\n",
    "            K.RandomPerspective(distortion_scale=0.3)\n",
    "        )\n",
    "        self.resize = K.Resize(size=(224, 224))\n",
    "        self.Normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.train_mode:\n",
    "            x = self.agument(input)\n",
    "        else:\n",
    "            x = input\n",
    "        x = self.resize(x)\n",
    "        x = self.Normalize(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79fbc3-f1b3-454a-839a-596a5f290e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, reduction='mean', weight=None, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.weight = weight\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, weight=self.weight,\n",
    "                                  reduction='none', label_smoothing=self.smoothing)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a99b8e-14ac-49b3-9197-6ffcfe7af581",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trn_ds = datasets.ImageFolder(trn_dir, transform=trans)\n",
    "vl_ds = datasets.ImageFolder(vl_dir, transform=trans)\n",
    "test_ds = datasets.ImageFolder(Test, transform=trans)\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "vl_dl = DataLoader(vl_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c9932-079c-4701-bf27-226c46bbf4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.resnet18(weights='DEFAULT')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = name.startswith('layer4') or name.startswith('fc')\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.to(device)\n",
    "\n",
    "criterion = FocalLoss(gamma=2, reduction='mean', smoothing=0.12)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.003)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210da20-6d09-452b-a72f-9684eff1cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(vl_dl, model):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in vl_dl:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            outputs = model(img)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe624cb8-428d-4ce6-ade7-ae8f7dba8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "best_val_acc = 0.0\n",
    "agment = Agumentation(train_mode=True).to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = total = correct = 0\n",
    "    for img, label in trn_dl:\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        img_agment = agment(img)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(\"cuda\"):\n",
    "            output = model(img_agment)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        total += label.size(0)\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    val_accuracy = eval(vl_dl, model)\n",
    "    print(f\"Epoch {epoch+1} --- Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Model saved at epoch {epoch+1} with val accuracy {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195b8ff-41f6-4f22-a837-ac2fffe772b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_dl:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f'Test Accuracy: {correct / total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fec5fb-f01f-4c80-843f-8951dbc442d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = [\n",
    "    0.9806, 0.9802, 0.9805, 0.9830, 0.9822, 0.9844, 0.9859, 0.9856, 0.9889, 0.9881,\n",
    "    0.9888, 0.9877, 0.9890, 0.9890, 0.9897, 0.9909, 0.9919, 0.9924, 0.9918, 0.9912,\n",
    "    0.9905, 0.9931, 0.9921, 0.9920, 0.9944, 0.9930, 0.9956, 0.9972, 0.9968, 0.9972,\n",
    "    0.9969, 0.9977, 0.9976, 0.9984, 0.9967\n",
    "]\n",
    "val_acc = [\n",
    "    0.9698, 0.9587, 0.9720, 0.9688, 0.9573, 0.9540, 0.9303, 0.9722, 0.9720, 0.9615,\n",
    "    0.9718, 0.9320, 0.9480, 0.9475, 0.9665, 0.9613, 0.9655, 0.9610, 0.9405, 0.9500,\n",
    "    0.9485, 0.9573, 0.9610, 0.9625, 0.9657, 0.9667, 0.9683, 0.9702, 0.9692, 0.9673,\n",
    "    0.9680, 0.9563, 0.9547, 0.9340, 0.9580\n",
    "]\n",
    "epochs = list(range(1, 36))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_acc, marker='o', linestyle='-', color='green', label='Train Accuracy')\n",
    "plt.plot(epochs, val_acc, marker='o', linestyle='-', color='blue', label='Validation Accuracy')\n",
    "plt.axvline(x=25, color='red', linestyle='--', label='Changed LR & Smoothing')\n",
    "plt.title('Train vs Validation Accuracy over 35 Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
